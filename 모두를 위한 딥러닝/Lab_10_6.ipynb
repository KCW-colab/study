{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-10-6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJiVwt07euAcLyUvKSlYhd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad9ae5fc86cb47fa8da9a962546aaa16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_60f066b8b21647eaa6320fd843db62f3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f4fe8b8b41d42d4855b74803ce056c2",
              "IPY_MODEL_18a47bd9ac0b480c8b18e104e4be7231"
            ]
          }
        },
        "60f066b8b21647eaa6320fd843db62f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f4fe8b8b41d42d4855b74803ce056c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8a386bd83c4149d683298f7fd2e3faef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5fef1c265316453c97275c713ec2d186"
          }
        },
        "18a47bd9ac0b480c8b18e104e4be7231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_315955240e294ff099c6402794ae41b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:19&lt;00:00, 8614183.94it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e9ab58d55d6497c894e57c8fbe2716c"
          }
        },
        "8a386bd83c4149d683298f7fd2e3faef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5fef1c265316453c97275c713ec2d186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "315955240e294ff099c6402794ae41b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e9ab58d55d6497c894e57c8fbe2716c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcypzvHh3xH5"
      },
      "source": [
        "# Lab-10-6-1 Advanced CNN(RESNET)1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ0YOmlY3uI7"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnDj5nza6EQn"
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "  '''3x3 convolution with padding'''\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size = 3, stride = stride,\n",
        "                   padding = 1, bias = True)\n",
        "  \n",
        "def conv1x1(in_planes, out_planes, stride = 1):\n",
        "  ''' 1x1 convolution '''\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride= stride, bias = False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs57Pqxf6ext"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample = None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    ''' stride 2라고 가정'''\n",
        "    # x.shape = 3 x 64 x 64\n",
        "    identity = x\n",
        "    # identity = 3 x 64 x 64\n",
        "    out = self.conv1(x) # 3 x 3 stride = stride\n",
        "    # out.shape = 3 x 32 x 32\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out) # 3 x 3 stride = 1\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    # out.shape = 3 x 32 x 32\n",
        "    # identity = 3 x 64 x 64\n",
        "    # 덧셈 불가 \n",
        "    # => downsample 사용\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)   # stride 가 2인 경우에 문제가 생김\n",
        "      # stride가 2인 경우는 주석으로 설명 처리 하겠음.\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "\n",
        "    return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXeQDO-a7dwi"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride = 1, downsample = None):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    self.conv1 = conv1x1(inplanes, planes) # conv1x1(64, 64)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = conv3x3(planes, planes, stride) # conv3x3(64, 64)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.conv3 = conv1x1(planes, planes * self.expansion) # conv1x1(64, 256)\n",
        "\n",
        "    self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "  \n",
        "    out = self.conv1(x) # 1x1 stride = 1\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)  # 3x3 stride = stride\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv3(out)   # 1x1 planes, planes * self.expansion\n",
        "    out = self.bn3(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "\n",
        "    return out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxLU-NeZ9vRG"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "  # model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) # resnet 50\n",
        "\n",
        "  def __init__(self, block, layers, num_classes = 1000, zero_init_residual=False):\n",
        "    super(ResNet, self).__init__()\n",
        "\n",
        "    self.inplanes = 64\n",
        "    # inputs = 3x224x224\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride =2, padding = 3, bias = False)\n",
        "    # outputs = self.conv1(inputs)\n",
        "    # outputs.shape = [64x112x112]\n",
        "    \n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace= True)\n",
        "\n",
        "    # inputs = [64x112x112]\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size = 3, stride =2 ,padding = 1)\n",
        "    # outputs = [64, 56, 56]\n",
        "\n",
        "    self.layer1 = self._make_layer(block, 64, layers[0]) # 3\n",
        "    self.layer2 = self._make_layer(block, 128, layers[1], stride = 2) # 4\n",
        "    self.layer3 = self._make_layer(block, 256, layers[2], stride = 2) # 6\n",
        "    self.layer4 = self._make_layer(block, 512, layers[3], stride = 2) # 3\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode = 'fan_out', nonlinearity = 'relu')\n",
        "\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "      # Zero_initialze the last BN in each residual branch,\n",
        "      # so that the residual branch starts with zeros, and each residual\n",
        "      # block behaves like an identity\n",
        "      # This improves the model by 0.2~0.3% according th https://arxiv.org/abs/1706.02677\n",
        "\n",
        "      if zero_init_residual:\n",
        "        for m in self.modules():\n",
        "          if isinstance(m, Bottleneck):\n",
        "            nn.init.constant_(m.bn3.weight, 0)\n",
        "          elif isinstance(m, BasicBlock):\n",
        "            nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "  # self.inplanes = 64          \n",
        "  # self.layer1 = self._make_layer(block, 64, layers[0]) # 3\n",
        "  def _make_layer(self, block, planes, blocks, stride = 1):\n",
        "    downsample = None\n",
        "\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "\n",
        "      downsample = nn.Sequential(\n",
        "          conv1x1(self.inplanes, planes * block.expansion, stride), # conv1x1(256, 512, 2)\n",
        "          nn.BatchNorm2d(planes * block.expansion), # batchnorm2d(512)\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "    # layers.append(BottleNeck(64, 64, 1, downsample))\n",
        "\n",
        "    self.inplanes = planes * block.expansion # self.inplanes = 256\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes)) # * 2\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPKWNpWCA68o"
      },
      "source": [
        "def resnet18(pretrained= False, **kwargs):\n",
        "  model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) # => 2*(2+2+2+2) +1(conv1) +1(fc) = 16 + 2 = resnet 18\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Wlb1YKBYGj"
      },
      "source": [
        "def resnet50(pretrained=False, **kwargs):\n",
        "  model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) # => 3*(3+4+6+3) +1(conv1) +1(fc) = 48 + 2 = 50\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1mwqITRBj6F"
      },
      "source": [
        "def resnet152(pretrained=False, **kwargs):\n",
        "  model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs) # 3*(3+8+36+3) + 2 = 150 + 2 = 152\n",
        "  return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI6GtPLIBtsx"
      },
      "source": [
        "res = resnet50()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRSjXwsnEfEC",
        "outputId": "8f8606fd-83a0-454a-cbad-1b12444be01c"
      },
      "source": [
        "res"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADmImzVcGGnH"
      },
      "source": [
        "# Lab-10-6-2 Advanced CNN(RESNET)2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fIrR8uPGF28"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbTYOaXOGanN",
        "outputId": "02ec931a-531f-4037-f007-2f58598fe07c"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "ad9ae5fc86cb47fa8da9a962546aaa16",
            "60f066b8b21647eaa6320fd843db62f3",
            "2f4fe8b8b41d42d4855b74803ce056c2",
            "18a47bd9ac0b480c8b18e104e4be7231",
            "8a386bd83c4149d683298f7fd2e3faef",
            "5fef1c265316453c97275c713ec2d186",
            "315955240e294ff099c6402794ae41b5",
            "3e9ab58d55d6497c894e57c8fbe2716c"
          ]
        },
        "id": "ULF2eNy4GmIm",
        "outputId": "a1d8fa29-ef07-4136-f036-6f7998f6f57d"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                 transforms.ToTensor()\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train = True, download = True, transform = transform)\n",
        "\n",
        "print(trainset.data.shape)\n",
        "\n",
        "train_data_mean = trainset.data.mean(axis=(0, 1, 2))\n",
        "train_data_std = trainset.data.std(axis=(0,1,2))\n",
        "\n",
        "print(train_data_mean)\n",
        "print(train_data_std)\n",
        "\n",
        "train_data_mean = train_data_mean / 255\n",
        "train_data_std = train_data_std / 25\n",
        "\n",
        "print(train_data_mean)\n",
        "print(train_data_std)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad9ae5fc86cb47fa8da9a962546aaa16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./cifar10/cifar-10-python.tar.gz to ./cifar10\n",
            "(50000, 32, 32, 3)\n",
            "[125.30691805 122.95039414 113.86538318]\n",
            "[62.99321928 62.08870764 66.70489964]\n",
            "[0.49139968 0.48215841 0.44653091]\n",
            "[2.51972877 2.48354831 2.66819599]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzP4OMzOHVnf",
        "outputId": "a487d80d-80b1-467c-c5c5-6e7ed4cd883e"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "                                      transforms.RandomCrop(32, padding = 4), \n",
        "                                      # padding을 4로 둘러주고 거기서 32만큼 랜덤하게 뜯어 오겠다.\n",
        "                                      # dataset을 여러가지로 (특이한 형태로) 가지고 올 수 있게함.\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(train_data_mean, train_data_std)\n",
        "\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(train_data_mean, train_data_std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root = './cifar10', train = True,\n",
        "                                        download = True, transform = transform_train)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 256, \n",
        "                                          shuffle = True, num_workers = 0)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root = './cifar10', train = False,\n",
        "                                        download = True, transform = transform_test)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(trainset, batch_size = 256, \n",
        "                                          shuffle = False, num_workers = 0)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpC3BJ8sItGi"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "  # model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) # resnet 50\n",
        "\n",
        "  def __init__(self, block, layers, num_classes = 1000, zero_init_residual=False):\n",
        "    super(ResNet, self).__init__()\n",
        "\n",
        "    self.inplanes = 16\n",
        "    # inputs = 3x224x224\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride =1, padding = 1, bias = False)\n",
        "    # outputs = self.conv1(inputs)\n",
        "    # outputs.shape = [64x112x112]\n",
        "    \n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.relu = nn.ReLU(inplace= True)\n",
        "\n",
        "    # inputs = [64x112x112]\n",
        "    # self.maxpool = nn.MaxPool2d(kernel_size = 3, stride =2 ,padding = 1)\n",
        "    # outputs = [64, 56, 56]\n",
        "\n",
        "    self.layer1 = self._make_layer(block, 16, layers[0], stride = 1) # 3\n",
        "    self.layer2 = self._make_layer(block, 32, layers[1], stride = 1) # 4\n",
        "    self.layer3 = self._make_layer(block, 64, layers[2], stride = 2) # 6\n",
        "    self.layer4 = self._make_layer(block, 128, layers[3], stride = 2) # 3\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.fc = nn.Linear(128 * block.expansion, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode = 'fan_out', nonlinearity = 'relu')\n",
        "\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "      # Zero_initialze the last BN in each residual branch,\n",
        "      # so that the residual branch starts with zeros, and each residual\n",
        "      # block behaves like an identity\n",
        "      # This improves the model by 0.2~0.3% according th https://arxiv.org/abs/1706.02677\n",
        "\n",
        "      if zero_init_residual:\n",
        "        for m in self.modules():\n",
        "          if isinstance(m, Bottleneck):\n",
        "            nn.init.constant_(m.bn3.weight, 0)\n",
        "          elif isinstance(m, BasicBlock):\n",
        "            nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "  # self.inplanes = 64          \n",
        "  # self.layer1 = self._make_layer(block, 64, layers[0]) # 3\n",
        "  def _make_layer(self, block, planes, blocks, stride = 1):\n",
        "    downsample = None\n",
        "\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "\n",
        "      downsample = nn.Sequential(\n",
        "          conv1x1(self.inplanes, planes * block.expansion, stride), # conv1x1(256, 512, 2)\n",
        "          nn.BatchNorm2d(planes * block.expansion), # batchnorm2d(512)\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "    # layers.append(BottleNeck(64, 64, 1, downsample))\n",
        "\n",
        "    self.inplanes = planes * block.expansion # self.inplanes = 256\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes)) # * 2\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x.shape = [batch, 3x32x32]\n",
        "    x = self.conv1(x)\n",
        "    # x.shape = [1, 16, 32, 32]\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.maxpool(x)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    # x.shape = [1, 64, 32, 32]\n",
        "    x = self.layer2(x)\n",
        "    # x.shape = [1, 128, 32, 32]\n",
        "    x = self.layer3(x)\n",
        "    # x.shape = [1, 256, 16, 16]\n",
        "    x = self.layer4(x)\n",
        "    # x.shape = [1, 512, 8, 8]\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Yx_lDnKSKV"
      },
      "source": [
        "resnet50 = ResNet(Bottleneck, [3, 4, 6, 3], 10, True).to(device)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAXlEIx5KZqw",
        "outputId": "682fe46f-6d94-4f5e-b68e-2ef5fcb617ff"
      },
      "source": [
        "resnet50"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix2GPzA8KcvH",
        "outputId": "68c83abc-ef46-4797-e311-281e00087a71"
      },
      "source": [
        "a = torch.Tensor(1, 3, 32, 32).to(device)\n",
        "out = resnet50(a)\n",
        "print(out)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSlpbTx-Kqpn"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(resnet50.parameters(), lr = 0.1, momentum = 0.9, \n",
        "                            weight_decay = 5e-4)\n",
        "\n",
        "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.5)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhiaeE7eLtIn"
      },
      "source": [
        "def acc_check(net, test_set, epoch, save=1):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_set:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = net(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    acc = (100 * correct / total)\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % acc)\n",
        "    if save:\n",
        "        torch.save(net.state_dict(), \"./model/model_epoch_{}_acc_{}.pth\".format(epoch, int(acc)))\n",
        "    return acc"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5vZzliUjMb7"
      },
      "source": [
        "import os\n",
        "os.mkdir('./model')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td5ZeqbyLaza",
        "outputId": "0a3a9e53-3759-4a20-f51f-44cc579eaad9"
      },
      "source": [
        "print(len(trainloader))\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = resnet50(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 30 == 29:    # print every 30 mini-batches\n",
        "            \n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 30))\n",
        "            running_loss = 0.0\n",
        "    lr_sche.step()\n",
        "    #Check Accuracy\n",
        "    acc = acc_check(resnet50, testloader, epoch, save=1)\n",
        "    \n",
        "    \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196\n",
            "[1,    30] loss: 2.091\n",
            "[1,    60] loss: 1.937\n",
            "[1,    90] loss: 1.796\n",
            "[1,   120] loss: 1.730\n",
            "[1,   150] loss: 1.647\n",
            "[1,   180] loss: 1.590\n",
            "Accuracy of the network on the 10000 test images: 43 %\n",
            "[2,    30] loss: 1.507\n",
            "[2,    60] loss: 1.443\n",
            "[2,    90] loss: 1.373\n",
            "[2,   120] loss: 1.359\n",
            "[2,   150] loss: 1.299\n",
            "[2,   180] loss: 1.254\n",
            "Accuracy of the network on the 10000 test images: 56 %\n",
            "[3,    30] loss: 1.160\n",
            "[3,    60] loss: 1.153\n",
            "[3,    90] loss: 1.133\n",
            "[3,   120] loss: 1.066\n",
            "[3,   150] loss: 1.048\n",
            "[3,   180] loss: 1.040\n",
            "Accuracy of the network on the 10000 test images: 65 %\n",
            "[4,    30] loss: 0.979\n",
            "[4,    60] loss: 0.961\n",
            "[4,    90] loss: 0.951\n",
            "[4,   120] loss: 0.968\n",
            "[4,   150] loss: 0.901\n",
            "[4,   180] loss: 0.902\n",
            "Accuracy of the network on the 10000 test images: 68 %\n",
            "[5,    30] loss: 0.867\n",
            "[5,    60] loss: 0.850\n",
            "[5,    90] loss: 0.859\n",
            "[5,   120] loss: 0.856\n",
            "[5,   150] loss: 0.835\n",
            "[5,   180] loss: 0.796\n",
            "Accuracy of the network on the 10000 test images: 72 %\n",
            "[6,    30] loss: 0.777\n",
            "[6,    60] loss: 0.772\n",
            "[6,    90] loss: 0.765\n",
            "[6,   120] loss: 0.743\n",
            "[6,   150] loss: 0.742\n",
            "[6,   180] loss: 0.759\n",
            "Accuracy of the network on the 10000 test images: 74 %\n",
            "[7,    30] loss: 0.726\n",
            "[7,    60] loss: 0.662\n",
            "[7,    90] loss: 0.672\n",
            "[7,   120] loss: 0.690\n",
            "[7,   150] loss: 0.665\n",
            "[7,   180] loss: 0.676\n",
            "Accuracy of the network on the 10000 test images: 78 %\n",
            "[8,    30] loss: 0.629\n",
            "[8,    60] loss: 0.640\n",
            "[8,    90] loss: 0.646\n",
            "[8,   120] loss: 0.613\n",
            "[8,   150] loss: 0.599\n",
            "[8,   180] loss: 0.643\n",
            "Accuracy of the network on the 10000 test images: 79 %\n",
            "[9,    30] loss: 0.605\n",
            "[9,    60] loss: 0.610\n",
            "[9,    90] loss: 0.593\n",
            "[9,   120] loss: 0.581\n",
            "[9,   150] loss: 0.565\n",
            "[9,   180] loss: 0.553\n",
            "Accuracy of the network on the 10000 test images: 81 %\n",
            "[10,    30] loss: 0.549\n",
            "[10,    60] loss: 0.566\n",
            "[10,    90] loss: 0.541\n",
            "[10,   120] loss: 0.543\n",
            "[10,   150] loss: 0.546\n",
            "[10,   180] loss: 0.550\n",
            "Accuracy of the network on the 10000 test images: 81 %\n",
            "[11,    30] loss: 0.451\n",
            "[11,    60] loss: 0.413\n",
            "[11,    90] loss: 0.425\n",
            "[11,   120] loss: 0.424\n",
            "[11,   150] loss: 0.435\n",
            "[11,   180] loss: 0.401\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[12,    30] loss: 0.386\n",
            "[12,    60] loss: 0.389\n",
            "[12,    90] loss: 0.386\n",
            "[12,   120] loss: 0.408\n",
            "[12,   150] loss: 0.404\n",
            "[12,   180] loss: 0.401\n",
            "Accuracy of the network on the 10000 test images: 88 %\n",
            "[13,    30] loss: 0.349\n",
            "[13,    60] loss: 0.362\n",
            "[13,    90] loss: 0.375\n",
            "[13,   120] loss: 0.395\n",
            "[13,   150] loss: 0.375\n",
            "[13,   180] loss: 0.411\n",
            "Accuracy of the network on the 10000 test images: 88 %\n",
            "[14,    30] loss: 0.350\n",
            "[14,    60] loss: 0.356\n",
            "[14,    90] loss: 0.360\n",
            "[14,   120] loss: 0.367\n",
            "[14,   150] loss: 0.382\n",
            "[14,   180] loss: 0.378\n",
            "Accuracy of the network on the 10000 test images: 88 %\n",
            "[15,    30] loss: 0.366\n",
            "[15,    60] loss: 0.338\n",
            "[15,    90] loss: 0.336\n",
            "[15,   120] loss: 0.355\n",
            "[15,   150] loss: 0.364\n",
            "[15,   180] loss: 0.359\n",
            "Accuracy of the network on the 10000 test images: 88 %\n",
            "[16,    30] loss: 0.350\n",
            "[16,    60] loss: 0.336\n",
            "[16,    90] loss: 0.341\n",
            "[16,   120] loss: 0.365\n",
            "[16,   150] loss: 0.357\n",
            "[16,   180] loss: 0.346\n",
            "Accuracy of the network on the 10000 test images: 89 %\n",
            "[17,    30] loss: 0.322\n",
            "[17,    60] loss: 0.331\n",
            "[17,    90] loss: 0.344\n",
            "[17,   120] loss: 0.357\n",
            "[17,   150] loss: 0.345\n",
            "[17,   180] loss: 0.360\n",
            "Accuracy of the network on the 10000 test images: 88 %\n",
            "[18,    30] loss: 0.317\n",
            "[18,    60] loss: 0.337\n",
            "[18,    90] loss: 0.333\n",
            "[18,   120] loss: 0.335\n",
            "[18,   150] loss: 0.350\n",
            "[18,   180] loss: 0.334\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "[19,    30] loss: 0.298\n",
            "[19,    60] loss: 0.314\n",
            "[19,    90] loss: 0.325\n",
            "[19,   120] loss: 0.338\n",
            "[19,   150] loss: 0.334\n",
            "[19,   180] loss: 0.331\n",
            "Accuracy of the network on the 10000 test images: 89 %\n",
            "[20,    30] loss: 0.311\n",
            "[20,    60] loss: 0.316\n",
            "[20,    90] loss: 0.317\n",
            "[20,   120] loss: 0.331\n",
            "[20,   150] loss: 0.328\n",
            "[20,   180] loss: 0.340\n",
            "Accuracy of the network on the 10000 test images: 90 %\n",
            "[21,    30] loss: 0.246\n",
            "[21,    60] loss: 0.216\n",
            "[21,    90] loss: 0.206\n",
            "[21,   120] loss: 0.209\n",
            "[21,   150] loss: 0.222\n",
            "[21,   180] loss: 0.211\n",
            "Accuracy of the network on the 10000 test images: 93 %\n",
            "[22,    30] loss: 0.181\n",
            "[22,    60] loss: 0.184\n",
            "[22,    90] loss: 0.180\n",
            "[22,   120] loss: 0.186\n",
            "[22,   150] loss: 0.203\n",
            "[22,   180] loss: 0.209\n",
            "Accuracy of the network on the 10000 test images: 94 %\n",
            "[23,    30] loss: 0.179\n",
            "[23,    60] loss: 0.177\n",
            "[23,    90] loss: 0.169\n",
            "[23,   120] loss: 0.190\n",
            "[23,   150] loss: 0.193\n",
            "[23,   180] loss: 0.201\n",
            "Accuracy of the network on the 10000 test images: 93 %\n",
            "[24,    30] loss: 0.184\n",
            "[24,    60] loss: 0.182\n",
            "[24,    90] loss: 0.176\n",
            "[24,   120] loss: 0.175\n",
            "[24,   150] loss: 0.182\n",
            "[24,   180] loss: 0.184\n",
            "Accuracy of the network on the 10000 test images: 94 %\n",
            "[25,    30] loss: 0.173\n",
            "[25,    60] loss: 0.158\n",
            "[25,    90] loss: 0.169\n",
            "[25,   120] loss: 0.175\n",
            "[25,   150] loss: 0.180\n",
            "[25,   180] loss: 0.198\n",
            "Accuracy of the network on the 10000 test images: 94 %\n",
            "[26,    30] loss: 0.168\n",
            "[26,    60] loss: 0.162\n",
            "[26,    90] loss: 0.173\n",
            "[26,   120] loss: 0.179\n",
            "[26,   150] loss: 0.166\n",
            "[26,   180] loss: 0.189\n",
            "Accuracy of the network on the 10000 test images: 94 %\n",
            "[27,    30] loss: 0.172\n",
            "[27,    60] loss: 0.155\n",
            "[27,    90] loss: 0.169\n",
            "[27,   120] loss: 0.168\n",
            "[27,   150] loss: 0.180\n",
            "[27,   180] loss: 0.175\n",
            "Accuracy of the network on the 10000 test images: 94 %\n",
            "[28,    30] loss: 0.151\n",
            "[28,    60] loss: 0.158\n",
            "[28,    90] loss: 0.163\n",
            "[28,   120] loss: 0.171\n",
            "[28,   150] loss: 0.181\n",
            "[28,   180] loss: 0.192\n",
            "Accuracy of the network on the 10000 test images: 95 %\n",
            "[29,    30] loss: 0.162\n",
            "[29,    60] loss: 0.166\n",
            "[29,    90] loss: 0.153\n",
            "[29,   120] loss: 0.169\n",
            "[29,   150] loss: 0.156\n",
            "[29,   180] loss: 0.171\n",
            "Accuracy of the network on the 10000 test images: 95 %\n",
            "[30,    30] loss: 0.148\n",
            "[30,    60] loss: 0.151\n",
            "[30,    90] loss: 0.171\n",
            "[30,   120] loss: 0.174\n",
            "[30,   150] loss: 0.185\n",
            "[30,   180] loss: 0.183\n",
            "Accuracy of the network on the 10000 test images: 94 %\n",
            "[31,    30] loss: 0.133\n",
            "[31,    60] loss: 0.092\n",
            "[31,    90] loss: 0.093\n",
            "[31,   120] loss: 0.080\n",
            "[31,   150] loss: 0.090\n",
            "[31,   180] loss: 0.084\n",
            "Accuracy of the network on the 10000 test images: 97 %\n",
            "[32,    30] loss: 0.071\n",
            "[32,    60] loss: 0.066\n",
            "[32,    90] loss: 0.061\n",
            "[32,   120] loss: 0.063\n",
            "[32,   150] loss: 0.073\n",
            "[32,   180] loss: 0.076\n",
            "Accuracy of the network on the 10000 test images: 98 %\n",
            "[33,    30] loss: 0.065\n",
            "[33,    60] loss: 0.063\n",
            "[33,    90] loss: 0.054\n",
            "[33,   120] loss: 0.059\n",
            "[33,   150] loss: 0.064\n",
            "[33,   180] loss: 0.065\n",
            "Accuracy of the network on the 10000 test images: 98 %\n",
            "[34,    30] loss: 0.064\n",
            "[34,    60] loss: 0.055\n",
            "[34,    90] loss: 0.060\n",
            "[34,   120] loss: 0.061\n",
            "[34,   150] loss: 0.066\n",
            "[34,   180] loss: 0.069\n",
            "Accuracy of the network on the 10000 test images: 98 %\n",
            "[35,    30] loss: 0.071\n",
            "[35,    60] loss: 0.063\n",
            "[35,    90] loss: 0.057\n",
            "[35,   120] loss: 0.064\n",
            "[35,   150] loss: 0.059\n",
            "[35,   180] loss: 0.065\n",
            "Accuracy of the network on the 10000 test images: 98 %\n",
            "[36,    30] loss: 0.059\n",
            "[36,    60] loss: 0.053\n",
            "[36,    90] loss: 0.056\n",
            "[36,   120] loss: 0.053\n",
            "[36,   150] loss: 0.055\n",
            "[36,   180] loss: 0.058\n",
            "Accuracy of the network on the 10000 test images: 98 %\n",
            "[37,    30] loss: 0.059\n",
            "[37,    60] loss: 0.056\n",
            "[37,    90] loss: 0.052\n",
            "[37,   120] loss: 0.061\n",
            "[37,   150] loss: 0.059\n",
            "[37,   180] loss: 0.068\n",
            "Accuracy of the network on the 10000 test images: 98 %\n",
            "[38,    30] loss: 0.055\n",
            "[38,    60] loss: 0.045\n",
            "[38,    90] loss: 0.059\n",
            "[38,   120] loss: 0.067\n",
            "[38,   150] loss: 0.064\n",
            "[38,   180] loss: 0.066\n",
            "Accuracy of the network on the 10000 test images: 98 %\n",
            "[39,    30] loss: 0.058\n",
            "[39,    60] loss: 0.057\n",
            "[39,    90] loss: 0.060\n",
            "[39,   120] loss: 0.060\n",
            "[39,   150] loss: 0.056\n",
            "[39,   180] loss: 0.061\n",
            "Accuracy of the network on the 10000 test images: 98 %\n",
            "[40,    30] loss: 0.057\n",
            "[40,    60] loss: 0.056\n",
            "[40,    90] loss: 0.062\n",
            "[40,   120] loss: 0.058\n",
            "[40,   150] loss: 0.059\n",
            "[40,   180] loss: 0.068\n",
            "Accuracy of the network on the 10000 test images: 98 %\n",
            "[41,    30] loss: 0.042\n",
            "[41,    60] loss: 0.034\n",
            "[41,    90] loss: 0.026\n",
            "[41,   120] loss: 0.026\n",
            "[41,   150] loss: 0.031\n",
            "[41,   180] loss: 0.030\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[42,    30] loss: 0.023\n",
            "[42,    60] loss: 0.020\n",
            "[42,    90] loss: 0.019\n",
            "[42,   120] loss: 0.020\n",
            "[42,   150] loss: 0.021\n",
            "[42,   180] loss: 0.019\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[43,    30] loss: 0.017\n",
            "[43,    60] loss: 0.016\n",
            "[43,    90] loss: 0.018\n",
            "[43,   120] loss: 0.016\n",
            "[43,   150] loss: 0.017\n",
            "[43,   180] loss: 0.017\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[44,    30] loss: 0.017\n",
            "[44,    60] loss: 0.014\n",
            "[44,    90] loss: 0.018\n",
            "[44,   120] loss: 0.016\n",
            "[44,   150] loss: 0.013\n",
            "[44,   180] loss: 0.015\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[45,    30] loss: 0.013\n",
            "[45,    60] loss: 0.013\n",
            "[45,    90] loss: 0.014\n",
            "[45,   120] loss: 0.016\n",
            "[45,   150] loss: 0.015\n",
            "[45,   180] loss: 0.017\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[46,    30] loss: 0.011\n",
            "[46,    60] loss: 0.011\n",
            "[46,    90] loss: 0.013\n",
            "[46,   120] loss: 0.013\n",
            "[46,   150] loss: 0.012\n",
            "[46,   180] loss: 0.014\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[47,    30] loss: 0.012\n",
            "[47,    60] loss: 0.011\n",
            "[47,    90] loss: 0.012\n",
            "[47,   120] loss: 0.012\n",
            "[47,   150] loss: 0.014\n",
            "[47,   180] loss: 0.014\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[48,    30] loss: 0.016\n",
            "[48,    60] loss: 0.015\n",
            "[48,    90] loss: 0.015\n",
            "[48,   120] loss: 0.015\n",
            "[48,   150] loss: 0.014\n",
            "[48,   180] loss: 0.015\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[49,    30] loss: 0.012\n",
            "[49,    60] loss: 0.010\n",
            "[49,    90] loss: 0.013\n",
            "[49,   120] loss: 0.012\n",
            "[49,   150] loss: 0.012\n",
            "[49,   180] loss: 0.013\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[50,    30] loss: 0.009\n",
            "[50,    60] loss: 0.010\n",
            "[50,    90] loss: 0.011\n",
            "[50,   120] loss: 0.011\n",
            "[50,   150] loss: 0.010\n",
            "[50,   180] loss: 0.011\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[51,    30] loss: 0.008\n",
            "[51,    60] loss: 0.008\n",
            "[51,    90] loss: 0.010\n",
            "[51,   120] loss: 0.008\n",
            "[51,   150] loss: 0.007\n",
            "[51,   180] loss: 0.009\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[52,    30] loss: 0.007\n",
            "[52,    60] loss: 0.007\n",
            "[52,    90] loss: 0.005\n",
            "[52,   120] loss: 0.006\n",
            "[52,   150] loss: 0.007\n",
            "[52,   180] loss: 0.007\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[53,    30] loss: 0.008\n",
            "[53,    60] loss: 0.007\n",
            "[53,    90] loss: 0.006\n",
            "[53,   120] loss: 0.008\n",
            "[53,   150] loss: 0.006\n",
            "[53,   180] loss: 0.006\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[54,    30] loss: 0.006\n",
            "[54,    60] loss: 0.007\n",
            "[54,    90] loss: 0.006\n",
            "[54,   120] loss: 0.006\n",
            "[54,   150] loss: 0.006\n",
            "[54,   180] loss: 0.006\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[55,    30] loss: 0.006\n",
            "[55,    60] loss: 0.006\n",
            "[55,    90] loss: 0.006\n",
            "[55,   120] loss: 0.005\n",
            "[55,   150] loss: 0.005\n",
            "[55,   180] loss: 0.005\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[56,    30] loss: 0.005\n",
            "[56,    60] loss: 0.005\n",
            "[56,    90] loss: 0.005\n",
            "[56,   120] loss: 0.006\n",
            "[56,   150] loss: 0.006\n",
            "[56,   180] loss: 0.006\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[57,    30] loss: 0.005\n",
            "[57,    60] loss: 0.005\n",
            "[57,    90] loss: 0.006\n",
            "[57,   120] loss: 0.006\n",
            "[57,   150] loss: 0.006\n",
            "[57,   180] loss: 0.005\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[58,    30] loss: 0.005\n",
            "[58,    60] loss: 0.005\n",
            "[58,    90] loss: 0.005\n",
            "[58,   120] loss: 0.006\n",
            "[58,   150] loss: 0.005\n",
            "[58,   180] loss: 0.005\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[59,    30] loss: 0.004\n",
            "[59,    60] loss: 0.005\n",
            "[59,    90] loss: 0.005\n",
            "[59,   120] loss: 0.004\n",
            "[59,   150] loss: 0.005\n",
            "[59,   180] loss: 0.005\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[60,    30] loss: 0.005\n",
            "[60,    60] loss: 0.005\n",
            "[60,    90] loss: 0.006\n",
            "[60,   120] loss: 0.005\n",
            "[60,   150] loss: 0.004\n",
            "[60,   180] loss: 0.005\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[61,    30] loss: 0.004\n",
            "[61,    60] loss: 0.005\n",
            "[61,    90] loss: 0.005\n",
            "[61,   120] loss: 0.004\n",
            "[61,   150] loss: 0.004\n",
            "[61,   180] loss: 0.005\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[62,    30] loss: 0.004\n",
            "[62,    60] loss: 0.005\n",
            "[62,    90] loss: 0.004\n",
            "[62,   120] loss: 0.005\n",
            "[62,   150] loss: 0.004\n",
            "[62,   180] loss: 0.004\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[63,    30] loss: 0.004\n",
            "[63,    60] loss: 0.004\n",
            "[63,    90] loss: 0.004\n",
            "[63,   120] loss: 0.004\n",
            "[63,   150] loss: 0.004\n",
            "[63,   180] loss: 0.004\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[64,    30] loss: 0.004\n",
            "[64,    60] loss: 0.004\n",
            "[64,    90] loss: 0.003\n",
            "[64,   120] loss: 0.004\n",
            "[64,   150] loss: 0.004\n",
            "[64,   180] loss: 0.003\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[65,    30] loss: 0.004\n",
            "[65,    60] loss: 0.004\n",
            "[65,    90] loss: 0.004\n",
            "[65,   120] loss: 0.004\n",
            "[65,   150] loss: 0.003\n",
            "[65,   180] loss: 0.003\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[66,    30] loss: 0.004\n",
            "[66,    60] loss: 0.004\n",
            "[66,    90] loss: 0.004\n",
            "[66,   120] loss: 0.003\n",
            "[66,   150] loss: 0.004\n",
            "[66,   180] loss: 0.004\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[67,    30] loss: 0.004\n",
            "[67,    60] loss: 0.004\n",
            "[67,    90] loss: 0.004\n",
            "[67,   120] loss: 0.004\n",
            "[67,   150] loss: 0.004\n",
            "[67,   180] loss: 0.004\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[68,    30] loss: 0.005\n",
            "[68,    60] loss: 0.004\n",
            "[68,    90] loss: 0.003\n",
            "[68,   120] loss: 0.004\n",
            "[68,   150] loss: 0.004\n",
            "[68,   180] loss: 0.004\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[69,    30] loss: 0.004\n",
            "[69,    60] loss: 0.004\n",
            "[69,    90] loss: 0.004\n",
            "[69,   120] loss: 0.004\n",
            "[69,   150] loss: 0.003\n",
            "[69,   180] loss: 0.005\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[70,    30] loss: 0.004\n",
            "[70,    60] loss: 0.004\n",
            "[70,    90] loss: 0.004\n",
            "[70,   120] loss: 0.005\n",
            "[70,   150] loss: 0.003\n",
            "[70,   180] loss: 0.004\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[71,    30] loss: 0.004\n",
            "[71,    60] loss: 0.003\n",
            "[71,    90] loss: 0.004\n",
            "[71,   120] loss: 0.003\n",
            "[71,   150] loss: 0.003\n",
            "[71,   180] loss: 0.003\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[72,    30] loss: 0.003\n",
            "[72,    60] loss: 0.003\n",
            "[72,    90] loss: 0.003\n",
            "[72,   120] loss: 0.003\n",
            "[72,   150] loss: 0.003\n",
            "[72,   180] loss: 0.003\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[73,    30] loss: 0.003\n",
            "[73,    60] loss: 0.004\n",
            "[73,    90] loss: 0.003\n",
            "[73,   120] loss: 0.003\n",
            "[73,   150] loss: 0.003\n",
            "[73,   180] loss: 0.003\n",
            "Accuracy of the network on the 10000 test images: 99 %\n",
            "[74,    30] loss: 0.004\n",
            "[74,    60] loss: 0.004\n",
            "[74,    90] loss: 0.003\n",
            "[74,   120] loss: 0.003\n",
            "[74,   150] loss: 0.003\n",
            "[74,   180] loss: 0.004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExDheyinLuzK"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = resnet50(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        \n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx5puKLgM0dk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}